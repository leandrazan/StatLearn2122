---
title: "Exercise 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## a) [0.5 points]
First of all, we read the data from the csv file and then run `str` to see the classes 
of the columns.

```{r}
heart <- read.csv( file = paste0("C:/Users/leaz9/OneDrive/Dokumente/StatLearn WS22", "/data/heart.csv"))
str(heart)
```




According to the description of the variables in the exercise, the columns `sex, cp, fbs, exang, ca, thal, hd` contain categorial variables. 
Since they are either of class `int` or `chr`, we transform them class `factor`. 
The function `mutate_at` from the `dplyr` package is very convenient for this task.

```{r, warning = FALSE, message=FALSE}
library(dplyr)
?mutate_at
heart <- mutate_at(heart, vars(sex, cp, fbs, exang, ca, thal, hd ), as.factor)
str(heart)
```
Of course, there are many ways to do it, you could e.g. also transform each column manually or use the `lapply()` function.


## b) [0.5 points]
Now we want to fit several logistic regression models. 
For this, we will use the function `glm` (generalized linear model).
The first paragraph in the section 'Details' of the function documentation of `glm`
states that, when fitting a logisitc regression model, the response variable (here: `hd`) can be a factor, and that in this case the second factor level is considered as success. 
Since here the second factor level is `` Unhealthy'' (see above) we are thus modelling
\( p(\mathbf{x}) = P(Unhealthy|\mathbf{x}) \).

We fit the three models with  `sex, sex` and `thal`, all features as explanatory variables as follows. Here, it is important to set `family = binomial`, such that indeed a logistic regression is performed.

```{r}
mod.sex <- glm(hd ~ sex  , data = heart, family = "binomial")
mod.sexthal <- glm(hd ~ sex + thal , data = heart, family = "binomial")
mod.full <-  glm(hd ~. , data = heart, family = "binomial")
```

For predicting the disease status based on the above models, we can use the 
`predict()` function. We have to specify `type ="response" ` to obtain estimates
of\( p(\mathbf{x})\) and not of logit\((p(\mathbf{x}))\), which is the default.

```{r}
pr.sex <- predict(mod.sex, newdata = heart, type = "response")
pr.sexthal <- predict(mod.sexthal, newdata = heart, type = "response")
pr.full <- predict(mod.full,newdata = heart, type = "response" )
```
Based on these estimated probabilities, we predict the individual to be unhealthy whenever the estimated probability is larger than or equal to 0.5. 
We do it for all three models at once using `lapply()`.
```{r}
pr.disease <- lapply(list( pr.sex =  pr.sex, pr.sexthal = pr.sexthal, 
                           pr.full = pr.full ), 
                     function(x) ifelse( x >= 0.5, "Unhealthy", "Healthy"))
```

From this, we can compute the in-sample misclassification rate. 
```{r}
lapply(pr.disease, function(x){ 
  conf_mat <- table(x, heart$hd)
  1 - sum(diag(conf_mat))/nrow(heart)})
```
The model using all features has by far the smallest in-sample error.

## c) [1 point]

We write a function that computes the out-of-sample error based on leaving-one-out cross-validation. The argument model can be one of full, sex, sexthal or reduced (for part (f)).
```{r}

compute_loocv <- function(index, model = "full", data){
  if( !(model %in% c("full", "sex", "sexthal", "reduced"))){stop("cv not implemented for this model.")}
  # reduce dataset according to features used in the respective model
  if(model == "sex"){ data <- select(data, c("sex", "hd"))}
  if(model == "sexthal"){ data <- select(data, c("sex", "thal", "hd"))}
  # for part (f)
  if(model == "reduced"){ 
    data <- select(data, -c("age", "chol", "fbs"))}
  
  data_train <- data[ -index, ]
  data_test <- data[ index, ]
  
  fitted.model <- glm(hd ~ . , data = data_train, family = "binomial")
  
  p.pred <- predict(fitted.model, newdata = data_test, type = "response")
  
  disease.pred <- ifelse(p.pred >= 0.5, "Unhealthy", "Healthy")
  disease.pred != data_test$hd  # returns TRUE ( = 1) whenever observation is misclassified
}
```

We apply the above function for every index in $\{ 1, \ldots, \mathrm{nrow(heart)} \}$ and compute the mean of the corresponding errors. 
```{r}
cv_sex <- mean(sapply( 1:nrow(heart), compute_loocv,  model = "sex",  data = heart))
cv_sexthal <- mean(sapply( 1:nrow(heart), compute_loocv,  model = "sexthal",  data = heart))
cv_full <- mean(sapply( 1:nrow(heart), compute_loocv,  model = "full",  data = heart))

cv_sex
cv_sexthal
cv_full

```


## d) [0.5 points]
We take a look at the estimated coefficients 
of the model fitted in (b)(ii)
```{r}
mod.sexthal$coefficients
```

The fitted model for the log odds is thus
\[ 
 \mathrm{logit}(p(\mathbf{x})) = \log(\mathrm{odds}(p(\mathbf{x}))) = 
 \beta_0 + \beta_1 \cdot 1(\mathrm{sex} =\, ''male'') + \beta_2\cdot 1(\mathrm{Thal} = 6) + \beta_3\cdot 1(\mathrm{Thal} = 7). \]
 
All coefficients (except intercept) are positive, i.e. being male and having thalium heart scan values of 6 or 7 is associated with an increase in log odds of being unhealthy. \\
Taking $\exp()$ on the euqation above, we see that
for fixed value of Thal, being male increases the odds of being unhealthy by \(\exp(\beta_1) \), 
while for fixed value of sex, having value "thal = 6" increases the odds of being unhealthy by \( \exp(\beta_2) \), 
and having value "thal = 7" increases the odds of being unhealthy by \(\exp(\beta_3) \).
These values are
```{r}
expbeta <- exp(mod.sexthal$coefficients[2:4])
names(expbeta) <- paste0("exp(beta", 1:3, ")")
expbeta
```



## e) [0.5 points]

```{r}
summary(mod.full)
```

The intercept, `age, chol` and `fbs1` might not have a big influence on the $\log$ odds, 
since the $p$-values of the tests with null hypothesis $H_0: \beta_j = 0$ are all larger than 0.05, i.e. the hypothesis that the corresponding feature does not have any effect on the $\log$ odds cannot be rejected at a level of 5\%. Note that this is a multiple testing problem, however, so $p$-values would have to be adjusted if you would want to rely on the test decisions. 

Furhter, `cp3` and `cp4` as well as `thal6` don't have significant $p$-values. 
One could therefore also try to only use the dummy variables `cp4` and `thal7` in the model.

## f) [0.5 points]
Reduced model:
```{r}
mod.red <- glm( hd ~ . - age - chol - fbs , 
                data = heart,  family = "binomial")

pr.red <- predict( mod.red, newdata = heart, type = "response")
pr.dis.red <- ifelse(pr.red >= 0.5,
                       "Unhealthy", "Healthy")
conf_red <- table(pr.dis.red, heart$hd)
1 - sum(diag(conf_red))/nrow(heart)  # in-sample misclassification rate

```

The computation of oos error based on LOOCV for the reduced model is already implemented in the function
`compute_loocv` above.

```{r}
cv_red <- mean(sapply( 1:nrow(heart), compute_loocv,  model = "reduced",  data = heart))
cv_red
cv_full

```

The reduced model has almost the same cv error as the full model, but it uses 3 features less.

## g) [0.5 points]
We assemble a data frame for making the plot.

```{r, fig.dim=c(6,4)}
library(ggplot2)
.df <- data.frame( prob = pr.red, status = heart$hd, x = rank(pr.red))

ggplot( .df, aes( x= x, y = prob, color = status)) + geom_point()+ 
  labs( x = "rank(p(x))", y = "p(x)")
```



